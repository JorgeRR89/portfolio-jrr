projects:
  - id: "taxi_demand"
    title: "Taxi Demand Forecasting — Sweet Lift"
    tagline: "Time-series demand forecasting for hourly rides; ops-ready features and evaluation."
    spotlight: true

    industry: "Mobility, Supply Chain & Optimization"
    type: "Time Series Forecasting"
    impact_type: "Cost & Efficiency"
    status: "Completed"
    year: "2025"

    # NEW (for GitHub enrichment + visual card)
    github_user: "JorgeRR89"
    github_repo: "Temporal-series-Sweet-Lift-Taxi"
    cover: "assets/projects/taxi_demand.png"

    skills:
      - Demand forecasting
      - Time series feature engineering
      - Model evaluation & backtesting
      - Operational analytics

    tools:
      - Python
      - pandas
      - scikit-learn
      - matplotlib

    outcomes:
      - "Hourly demand forecasting with feature engineering (lags/rolling/calendar)"
      - "Time-aware train/test split (past → future)"
      - "RMSE evaluation + ops-friendly interpretation"

    problem: "Operations need reliable short-horizon demand forecasts to plan driver capacity efficiently."
    approach: "Resample/clean time series → build lag/rolling/calendar features → train baseline + ML models → evaluate with RMSE on a future holdout."
    results: "Improved forecast stability and produced an ops-friendly evaluation view for peak-hour staffing decisions."
    details: "Built as a reproducible notebook workflow with clear plots and metrics."

    links:
      github: "https://github.com/JorgeRR89/Temporal-series-Sweet-Lift-Taxi"
      colab: ""
      demo: ""
      report: ""

    lab:
      mode: "time_series"
      demo_asset: "data/lab/taxi_demo.csv"
      
  - id: "good_seed_cv"
    title: "Computer Vision — Good Seed Supermarket"
    tagline: "AI vision system to estimate customer age and support safe, automated retail operations."
    spotlight: true

    industry: "Retail, Computer Vision & Intelligent Systems"
    type: "Computer Vision / Deep Learning (Regression)"
    impact_type: "Compliance, Risk & Operational Efficiency"
    status: "Completed"
    year: "2025"

    # Repo integration
    github_user: "JorgeRR89"
    github_repo: "Visi-n-Artificial---Supermercado-Good-Seed"
    cover: "assets/projects/good_seed_cv.png"

    skills:
      - Computer vision pipelines
      - Deep learning with CNNs
      - Transfer learning (ResNet50)
      - Data augmentation & preprocessing
      - Model evaluation & error analysis
      - Applied AI for operations

    tools:
      - Python
      - NumPy
      - pandas
      - TensorFlow / Keras
      - matplotlib

    outcomes:
      - "Built an age-estimation model from facial images using transfer learning"
      - "Reduced MAE from ~17.4 → ~11.0 years with stable convergence"
      - "Designed a deployable vision pipeline with GPU-efficient training"
      - "Delivered operational framing for safe AI-assisted age verification"

    problem: "Retail operations involving age-restricted products rely on manual verification, creating friction, inconsistency, and legal risk."
    approach: "Preprocess and augment images → use pretrained ResNet50 as feature extractor → add dense regression head → train with MAE loss and Adam optimizer → apply EarlyStopping and LR scheduling → evaluate MAE/MSE and error behavior."
    results: "Final validation MAE ≈ 11.05 years and MSE ≈ 191.3, demonstrating a stable deep learning system capable of supporting automated age estimation with human-in-the-loop validation."
    details: "GPU training (~35 minutes on NVIDIA T4). Architecture: ResNet50 + GAP → Dense(128, ReLU) → Dropout(0.3) → Dense(1). Dataset split 80/20 with ImageDataGenerator and ImageNet preprocessing."

    links:
      github: "https://github.com/JorgeRR89/Visi-n-Artificial---Supermercado-Good-Seed"
      colab: ""
      demo: ""
      report: ""

    lab:
      mode: "none"
      demo_asset: ""

  - id: "film_junky_union"
    title: "Sentiment Analysis Platform — Film Junky Union"
    tagline: "NLP system to automatically classify movie reviews as positive or negative with high reliability."
    spotlight: false

    industry: "Media, Entertainment & Platforms"
    type: "Natural Language Processing / Classification"
    impact_type: "Automation & Decision Support"
    status: "Completed"
    year: "2025"
    
    # Repo integration
    github_user: "JorgeRR89"
    github_repo: "Aprendizaje-automatico---Film-Junky-Union"
    cover: "assets/covers/film_junky.png"

    skills:
      - Natural Language Processing (NLP)
      - Text cleaning & normalization
      - Feature engineering (TF-IDF)
      - Supervised machine learning
      - Model evaluation & validation
      - Applied sentiment analysis

    tools:
      - Python
      - pandas
      - scikit-learn
      - spaCy
      - NLTK
      - LightGBM

    outcomes:
      - "Built an automated sentiment classification system for IMDB movie reviews."
      - "Achieved F1 = 0.89 and ROC AUC = 0.96 on unseen test data."
      - "Designed and compared multiple NLP pipelines (baseline → logistic regression → gradient boosting)."
      - "Delivered a production-viable model balancing performance, interpretability, and speed."

    problem: >
      Film Junky Union needed a scalable way to classify large volumes of movie reviews as positive or negative.
      Manual moderation and analysis were not feasible, limiting the platform’s ability to filter content,
      understand audience perception, and prioritize critical feedback.

    approach: >
      Conducted exploratory analysis and built a full NLP pipeline including text cleaning, normalization,
      and lemmatization. Generated TF-IDF representations and trained multiple classifiers
      (Logistic Regression and LightGBM). Evaluated models using Accuracy, F1-score, ROC AUC, and
      Average Precision, selecting the most balanced solution.

    results: >
      The final model (spaCy lemmatization + TF-IDF + Logistic Regression) reached F1 = 0.89 and ROC AUC = 0.96,
      clearly outperforming the baseline and meeting the business requirement.
      The system reliably detects negative and positive sentiment while remaining fast and interpretable.

    details: >
      Implemented time-efficient text pipelines, compared traditional ML and boosting methods,
      and validated generalization on a held-out test set.
      The project demonstrates how applied NLP can transform unstructured text into operational signals.

    links:
      github: "https://github.com/JorgeRR89/Aprendizaje-automatico---Film-Junky-Union"
      colab: ""
      demo: ""
      report: ""

    lab:
      mode: "nlp_classification"
      demo_asset: ""

  - id: "ctr_prediction"
    title: "CTR Prediction Modeling — MIT xPRO"
    tagline: "Predictive modeling to estimate ad CTR and improve marketing decision systems with strong out-of-sample performance."
    spotlight: true

    industry: "Marketing, Growth & Revenue Systems"
    type: "CTR Prediction • Regression Modeling"
    impact_type: "Revenue & Optimization"
    status: "Completed"
    year: "2025"

    # Repo integration (for auto-readme / metadata / cover)
    github_user: "JorgeRR89"
    github_repo: "CTR"
    cover: "assets/projects/ctr.png" (recommended)
    skills:
      - CTR modeling
      - One-hot encoding & feature prep
      - Model comparison & selection
      - Bias/variance control (pruning)
      - Out-of-sample evaluation

    tools:
      - Python
      - pandas
      - numpy
      - scikit-learn
      - xgboost

    outcomes:
      - "Benchmarked Linear Regression, CART, Random Forest and XGBoost for CTR prediction"
      - "Reduced overfitting via cost-complexity pruning and controlled model complexity"
      - "Selected best generalizing model using OSR², MAE and RMSE on a holdout test set"

    problem: "Marketing teams need reliable CTR predictions to optimize targeting, bidding, and budget allocation on new ad impressions."
    approach: "Prepared train/test datasets → one-hot encoded categorical features (e.g., age, gender) → trained multiple regression models → tuned complexity (pruning) → compared out-of-sample metrics (OSR², MAE, RMSE) → selected best generalizing model."
    results: "Best model achieved OSR² ≈ 0.493 with out-of-sample MAE ≈ 0.031 and RMSE ≈ 0.054, providing a solid foundation for CTR-driven optimization decisions."
    details: "Built and evaluated in Python (Colab) with transparent metric reporting and model comparison."

    links:
      github: "https://github.com/JorgeRR89/CTR"
      colab: ""
      demo: ""
      report: ""

    lab:
      mode: "regression"
      demo_asset: ""


